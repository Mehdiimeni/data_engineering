# Data Engineering Pipeline Project

## Overview

This project demonstrates a data engineering pipeline using MySQL, Python (with the Faker library), and PySpark. It involves generating and populating fake data into a MySQL database, processing the data in batches with PySpark, and organizing the workflow using Docker and Docker Compose.

---

## Project Structure

```
+data_generator
    - data_faker.py         # Python script to generate and insert fake data into MySQL
    - Dockerfile            # Dockerfile for data_generator service

+db
    - eer.mwb               # EER diagram for database schema

+libs
    - mysql-connector-j-9.1.0.jar  # MySQL JDBC connector for PySpark

+pyspark_app
    - config.yaml           # Configuration file for PySpark
    - py_spark_app.py       # PySpark script for batch processing
    - Dockerfile            # Dockerfile for pyspark_app service

+venv                      # Virtual environment for local development (optional)
.env                       # Virtual environment configuration
.gitignore                 # Git ignore file
docker-compose.yml         # Docker Compose configuration
README.md                  # Project documentation
```

---

## Services

1. MySQL  
   - Stores the data generated by the `data_generator` service.  
   - Database: `data_engineering`.

2. Data Generator  
   - Generates fake data for products, orders, and their relationships using the Faker library.  
   - Populates the MySQL database with realistic test data.

3. PySpark Application  
   - Reads data from the MySQL database in batch intervals.  
   - Processes and transforms the data into a nested structure for further analytics or storage.

---

## How to Run the Project

### Prerequisites
- Docker and Docker Compose installed on your system.

### Steps
1. Clone the repository:
   ```bash
   git clone <repository-url>
   cd <repository-directory>
   ```

2. Build and start all services:
   ```bash
   docker-compose up --build
   ```

3. Verify services:
   - MySQL is available on port `3306`.
   - The `data_generator` service populates data into MySQL.
   - The `pyspark_app` service processes data from MySQL in batches.

---

## Key Features

- Data Generation: Uses the Faker library to generate random data for products, orders, and order-product relationships.
- Batch Processing: Processes data in hourly intervals using PySpark.
- Containerized Deployment: Docker ensures consistent runtime environments for all services.

---

## Technologies Used

- Python: For data generation and integration.
- MySQL: To store structured data.
- PySpark: For data processing and transformation.
- Docker & Docker Compose: For containerized service orchestration.

---

## Future Enhancements

- Add a visualization dashboard to display processed data.
- Implement real-time streaming for live data processing.
- Extend the pipeline to integrate with a data lake or warehouse.

---

## Contact

For any inquiries or contributions, feel free to reach out to imeni1982@gmail.com .

